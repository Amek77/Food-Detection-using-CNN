{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5ac2d7-70b7-4a7e-9c27-56aee5a5567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5402c25c-aa3f-41b7-a81a-89ffd019ed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = \"data/train\" \n",
    "classes = [\"burger\", \"masala_dosa\", \"momos\", \"pizza\", \"samosa\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a32918b-932d-4ece-b9bf-1862c55b4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_dict = {}\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(img_path, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        img_path_dict[filename] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be43fc36-0c08-452c-a977-75e225325dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(img_path_dict.items()), columns=[\"filename\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9a73437-a5df-4c10-a581-4d3c1e928328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(metadata_path, img_dir, img_size=(30, 30)):\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_names = metadata['class'].unique()\n",
    "    class_dict = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "    \n",
    "    for index, row in metadata.iterrows():\n",
    "        img_path = os.path.join(img_dir, row['class'], row['filename'])\n",
    "        img = Image.open(img_path).resize(img_size)\n",
    "        img = img.convert('RGB')\n",
    "        arr = np.array(img)\n",
    "        images.append(arr)\n",
    "        labels.append(class_dict[row['class']])\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ee3ca-99ba-45ea-873c-1a8e6f0b99c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata_path = \"data/Metafile1.csv\"\n",
    "x_train, y_train = load_images(metadata_path, img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a18624-66c4-47fc-bc9f-8f8bee5c0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f653c57-ac54-405d-989b-9d30ea68e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a8e7ff-b396-478a-91e0-b52285c202a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41182911-919b-4c16-910c-2678f7817f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b6ee17-d299-4665-b4f9-95360b034c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aarus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(30, 30, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(classes), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3dc4c8-5e3c-4ccd-b3cc-1f1df64e599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edc90f-089a-46e4-b412-2968278e72f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - accuracy: 0.2227 - loss: 1.6089 - val_accuracy: 0.2764 - val_loss: 1.5894\n",
      "Epoch 2/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.3519 - loss: 1.5543 - val_accuracy: 0.3869 - val_loss: 1.4546\n",
      "Epoch 3/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3653 - loss: 1.4776 - val_accuracy: 0.4070 - val_loss: 1.3532\n",
      "Epoch 4/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4879 - loss: 1.2934 - val_accuracy: 0.4070 - val_loss: 1.4066\n",
      "Epoch 5/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4737 - loss: 1.2703 - val_accuracy: 0.5427 - val_loss: 1.2014\n",
      "Epoch 6/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.5812 - loss: 1.1045 - val_accuracy: 0.5578 - val_loss: 1.2195\n",
      "Epoch 7/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5864 - loss: 1.0431 - val_accuracy: 0.5930 - val_loss: 1.0325\n",
      "Epoch 8/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6372 - loss: 0.9837 - val_accuracy: 0.5628 - val_loss: 1.0334\n",
      "Epoch 9/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6907 - loss: 0.8440 - val_accuracy: 0.5980 - val_loss: 0.9880\n",
      "Epoch 10/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6954 - loss: 0.8103 - val_accuracy: 0.6181 - val_loss: 0.9635\n",
      "Epoch 11/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7028 - loss: 0.7857 - val_accuracy: 0.5879 - val_loss: 0.9826\n",
      "Epoch 12/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7336 - loss: 0.7406 - val_accuracy: 0.6281 - val_loss: 0.9539\n",
      "Epoch 13/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7751 - loss: 0.6196 - val_accuracy: 0.6432 - val_loss: 0.9160\n",
      "Epoch 14/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7731 - loss: 0.6195 - val_accuracy: 0.5628 - val_loss: 1.1395\n",
      "Epoch 15/15\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7600 - loss: 0.6587 - val_accuracy: 0.6332 - val_loss: 0.9653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.6242 - loss: 1.0054\n",
      "Validation Accuracy: 63.32%\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset)\n",
    "\n",
    "model.save(\"models/food_detection_model.h5\")\n",
    "\n",
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n",
    "\n",
    "def predict_food(image_path, model, img_size=(30, 30)):\n",
    "    img = Image.open(image_path).resize(img_size)\n",
    "    img = img.convert('RGB')\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0) \n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_class = np.argmax(prediction, axis=1)\n",
    "    return classes[predicted_class[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d859867-6d8b-4232-ba60-dc98e47f0ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/my_model.keras\")\n",
    "model = tf.keras.models.load_model('models/my_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "403069a8-34e7-4f3f-8bd0-ad99a9987f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template_string\n",
    "from pyngrok import ngrok\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be75d9-00d2-4294-ba6b-28ecd6db7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aarus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: NgrokTunnel: \"https://22994da991b7.ngrok-free.app\" -> \"http://localhost:5000\"\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.3:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [04/Jan/2026 18:30:15] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jan/2026 18:30:15] \"GET /static/ep1.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Jan/2026 18:30:16] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [04/Jan/2026 18:32:43] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [04/Jan/2026 18:32:43] \"GET /static/ep1.jpg HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 569ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Jan/2026 18:32:57] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.models.load_model(\"models/my_model.keras\")\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = image.resize((30, 30))\n",
    "    image = image.convert('RGB')\n",
    "    image = np.array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def upload_form():\n",
    "    return render_template_string(\"\"\"\n",
    "  <!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Food Detection System</title>\n",
    "    <style>\n",
    "         body {\n",
    "            font-family: 'Arial', sans-serif;\n",
    "            margin: 0;\n",
    "            padding: 0;\n",
    "            background-color: #f4f4f9;\n",
    "            background-image:url(\"static/ep1.jpg\");\n",
    "            color: #333;\n",
    "            \n",
    "            background-size: 1600px 800px;\n",
    "        }\n",
    "        .navbar {\n",
    "            background-color: #333;\n",
    "            overflow: hidden;\n",
    "            padding: 10px 20px;\n",
    "            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .navbar a {\n",
    "            float: left;\n",
    "            display: block;\n",
    "            color: #f2f2f2;\n",
    "            text-align: center;\n",
    "            padding: 14px 16px;\n",
    "            text-decoration: none;\n",
    "            font-size: 18px;\n",
    "        }\n",
    "        .navbar a:hover {\n",
    "            background-color: #575757;\n",
    "            border-radius: 5px;\n",
    "        }\n",
    "        .container {\n",
    "            text-align: center;\n",
    "            padding: 20px;\n",
    "            max-width: 800px;\n",
    "            margin: 0 auto;\n",
    "        }\n",
    "        h1 {\n",
    "            font-size: 3em;\n",
    "            color: #ffd700;\n",
    "            margin-bottom: 20px;\n",
    "            text-shadow: 2px 2px 4px rgba(0, 0, 0, 1);\n",
    "        }\n",
    "        .dropbox {\n",
    "            border: 2px dashed #888;\n",
    "            padding: 30px;\n",
    "            border-radius: 10px;\n",
    "            background-color: #fafafa;\n",
    "            cursor: pointer;\n",
    "            margin-top: 20px;\n",
    "            transition: background-color 0.3s, border-color 0.3s;\n",
    "        }\n",
    "        .dropbox.dragover {\n",
    "            border-color: #555;\n",
    "            background-color: #e8e8e8;\n",
    "        }\n",
    "        #uploaded-image {\n",
    "            max-width: 100%;\n",
    "            margin-top: 20px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .result {\n",
    "            margin-top: 20px;\n",
    "            font-size: 1.2em;\n",
    "            color: #444;\n",
    "            padding: 10px;\n",
    "            background-color: #e0f7fa;\n",
    "            border-radius: 10px;\n",
    "        }\n",
    "        .about-cnn {\n",
    "            margin-top: 5px;\n",
    "            text-align: left;\n",
    "            background-color:rgba(red, green, blue, 0);\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .about-cnn h2 {\n",
    "            color: #f3f30f;\n",
    "            margin-bottom: 10px;\n",
    "            text-shadow: 2px 4px 6px rgba(0, 0, 0, 0.5);\n",
    "        }\n",
    "        .about-cnn p {\n",
    "            font-size: 1.1em;\n",
    "            line-height: 1.6;\n",
    "            color: #ebeb4c;\n",
    "            text-shadow: 2px 4px 6px rgba(0, 0, 0, 1);\n",
    "        }\n",
    "        canvas {\n",
    "            position: fixed;\n",
    "            top: 0;\n",
    "            left: 0;\n",
    "            z-index: -1;\n",
    "            width: 100%;\n",
    "            height: 100%;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <nav class=\"navbar\">\n",
    "        <a href=\"#\">Home</a>\n",
    "        <a href=\"#about\">About CNN</a>\n",
    "        <a href=\"#upload\">Upload Image</a>\n",
    "    </nav>\n",
    "    <canvas id=\"background-canvas\"></canvas>\n",
    "    <div class=\"container\">\n",
    "        <h1>Food Detection System using CNN</h1>\n",
    "        <div id=\"upload\">\n",
    "            <div id=\"dropbox\" class=\"dropbox\">Drag and drop an image here or click to select</div>\n",
    "            <input type=\"file\" id=\"file-input\" accept=\"image/*\" style=\"display: none;\">\n",
    "            <img id=\"uploaded-image\" src=\"\" alt=\"Uploaded Image\" hidden>\n",
    "            <div id=\"result\" class=\"result\" hidden></div>\n",
    "        </div>\n",
    "        <div class=\"about-cnn\" id=\"about\">\n",
    "            <h2>About Convolutional Neural Networks (CNN)</h2>\n",
    "            <p>\n",
    "                Convolutional Neural Networks (CNNs) are a class of deep learning models specifically designed for image processing tasks. They use convolutional layers to automatically and adaptively learn spatial hierarchies of features from input images. CNNs are widely used in applications like image classification, object detection, and facial recognition.\n",
    "            </p>\n",
    "            <p>\n",
    "                In this Food Detection System, a CNN model is trained to classify food images into different categories. The model processes the input image, extracts features, and predicts the most likely food class.\n",
    "            </p>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        const dropbox = document.getElementById('dropbox');\n",
    "        const fileInput = document.getElementById('file-input');\n",
    "        const uploadedImage = document.getElementById('uploaded-image');\n",
    "        const resultDiv = document.getElementById('result');\n",
    "\n",
    "        dropbox.addEventListener('click', () => fileInput.click());\n",
    "\n",
    "        dropbox.addEventListener('dragover', (event) => {\n",
    "            event.preventDefault();\n",
    "            dropbox.classList.add('dragover');\n",
    "        });\n",
    "\n",
    "        dropbox.addEventListener('dragleave', () => dropbox.classList.remove('dragover'));\n",
    "\n",
    "        dropbox.addEventListener('drop', (event) => {\n",
    "            event.preventDefault();\n",
    "            dropbox.classList.remove('dragover');\n",
    "            const file = event.dataTransfer.files[0];\n",
    "            if (file) handleFile(file);\n",
    "        });\n",
    "\n",
    "        fileInput.addEventListener('change', () => {\n",
    "            const file = fileInput.files[0];\n",
    "            if (file) handleFile(file);\n",
    "        });\n",
    "\n",
    "        function handleFile(file) {\n",
    "            if (!file.type.startsWith('image/')) {\n",
    "                alert('Please upload a valid image file.');\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            const reader = new FileReader();\n",
    "            reader.onload = () => {\n",
    "                uploadedImage.src = reader.result;\n",
    "                uploadedImage.hidden = false;\n",
    "                sendImageToServer(file);\n",
    "            };\n",
    "            reader.readAsDataURL(file);\n",
    "        }\n",
    "\n",
    "        function sendImageToServer(file) {\n",
    "            const formData = new FormData();\n",
    "            formData.append('image', file);\n",
    "\n",
    "            fetch(`${window.location.origin}/predict`, {\n",
    "                method: 'POST',\n",
    "                body: formData\n",
    "            })\n",
    "                .then(response => response.json())\n",
    "                .then(data => {\n",
    "                    resultDiv.textContent = `The Predicted Class is ${data.class}`;\n",
    "                    resultDiv.hidden = false;\n",
    "                })\n",
    "                .catch(error => {\n",
    "                    resultDiv.textContent = 'Error: Unable to make prediction.';\n",
    "                    resultDiv.hidden = false;\n",
    "                    console.error(error);\n",
    "                });\n",
    "        }\n",
    "\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "    \n",
    "                \n",
    "        \n",
    "    \"\"\")\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    file = request.files['image']\n",
    "    image = Image.open(file.stream)\n",
    "    processed_image = preprocess_image(image)\n",
    "    prediction = model.predict(processed_image)\n",
    "    predicted_class = classes[np.argmax(prediction)]\n",
    "    return jsonify({'class': predicted_class})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ngrok_token = os.getenv('NGROK_AUTH_TOKEN')\n",
    "    if ngrok_token:\n",
    "        ngrok.set_auth_token(ngrok_token)\n",
    "        public_url = ngrok.connect(5000)\n",
    "        print(f\"Public URL: {public_url}\")\n",
    "    else:\n",
    "        print(\"Warning: NGROK_AUTH_TOKEN not set. Running without ngrok.\")\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66740835-b569-478b-8541-c748970f5b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
